{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Financial news categorization/sentiment analysis using NLP techniques\n",
    "\n",
    "\n",
    "Sentiment analysis is the statistical analysis of simple sentiment\n",
    "cues. Essentially, it involves making statistical analyses on polarized\n",
    "statements (i.e., statements with a positive, negative and neutral sentiment), which are usually collected in the form of social media posts,\n",
    "reviews, and news articles. Financial sentiment analysis is a challenging task due to the specialized language and lack of labeled data in that domain.\n",
    "\n",
    "\n",
    "In our case, we will focus on two different tasks.\n",
    "\n",
    "\n",
    "1. **Category tagger**: Create a NLP classifier capable of assigning a financial category to a text derived from the financial industry.\n",
    "\n",
    "The Twitter Financial News dataset is an English-language dataset containing an annotated corpus of finance-related tweets. This dataset is used to classify finance-related tweets for their topic.\n",
    "\n",
    "    The dataset holds 21,107 documents annotated with 20 labels:\n",
    "\n",
    "```json\n",
    "topics = {\n",
    "    \"LABEL_0\": \"Analyst Update\",\n",
    "    \"LABEL_1\": \"Fed | Central Banks\",\n",
    "    \"LABEL_2\": \"Company | Product News\",\n",
    "    \"LABEL_3\": \"Treasuries | Corporate Debt\",\n",
    "    \"LABEL_4\": \"Dividend\",\n",
    "    \"LABEL_5\": \"Earnings\",\n",
    "    \"LABEL_6\": \"Energy | Oil\",\n",
    "    \"LABEL_7\": \"Financials\",\n",
    "    \"LABEL_8\": \"Currencies\",\n",
    "    \"LABEL_9\": \"General News | Opinion\",\n",
    "    \"LABEL_10\": \"Gold | Metals | Materials\",\n",
    "    \"LABEL_11\": \"IPO\",\n",
    "    \"LABEL_12\": \"Legal | Regulation\",\n",
    "    \"LABEL_13\": \"M&A | Investments\",\n",
    "    \"LABEL_14\": \"Macro\",\n",
    "    \"LABEL_15\": \"Markets\",\n",
    "    \"LABEL_16\": \"Politics\",\n",
    "    \"LABEL_17\": \"Personnel Change\",\n",
    "    \"LABEL_18\": \"Stock Commentary\",\n",
    "    \"LABEL_19\": \"Stock Movement\"\n",
    "}\n",
    "```\n",
    "\n",
    "2. **Sentiment tagger**: Create a NLP classifier capable of assigning a sentiment score (positive,negative,neutral) to text derived from the financial industry. Additionally, we will use a powerful pre-trained model, finetuned on financial data, to assign scores to financial headlines, data from social media posts, etc ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites:\n",
    "\n",
    "\n",
    "High level requirements of Python library.\n",
    "\n",
    "    - Pytorch\n",
    "    - HuggingFace Transformers library\n",
    "    - Pandas\n",
    "    - Numpy\n",
    "    - Sklearn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda create --name finsent-env python=3.11  \n",
    "# conda activate finset-env\n",
    "# conda install pip\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install transformers\n",
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pickle\n",
    "import torch\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import \\\n",
    "    AutoModelForSequenceClassification, AutoTokenizer, \\\n",
    "    BertForSequenceClassification, BertTokenizer, BertForSequenceClassification, \\\n",
    "    AdamW, BertConfig, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 1: Pulling the data together**\n",
    "\n",
    "\n",
    "Download and inspect the data from the various sources:\n",
    "\n",
    "1. Financial Phrasebank https://huggingface.co/datasets/financial_phrasebank. Humanly annotated - sentiment\n",
    "\n",
    "2. Financial tweets topics dataset: https://huggingface.co/datasets/zeroshot/twitter-financial-news-topic/viewer/default/train?p=169. Humanly annotated - Category\n",
    "\n",
    "Think of any pre-processing functions (\n",
    "    Converting the text to lowercase,\n",
    "    removing punctuation,\n",
    "    tokenizing the text,\n",
    "    removing stop words and empty strings,\n",
    "    lemmatizing tokens.\n",
    ") that you might need to apply for downstream tasks. As always, pick a framework for data analysis and data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading sentences\n",
    "sentences_50 = \"../data/raw/Finanical-Phrases-Sentiment/Sentences_50Agree.txt\"\n",
    "sentences_66 = \"../data/raw/Finanical-Phrases-Sentiment/Sentences_66Agree.txt\"\n",
    "sentences_75 = \"../data/raw/Finanical-Phrases-Sentiment/Sentences_75Agree.txt\"\n",
    "sentences_All= \"../data/raw/Finanical-Phrases-Sentiment/Sentences_AllAgree.txt\"\n",
    "\n",
    "# Saving in variable\n",
    "files=[sentences_50, sentences_66, sentences_75, sentences_All]\n",
    "\n",
    "# Function to get file data\n",
    "def get_file_data(file):\n",
    "\n",
    "    df = pd.read_csv(file, sep=\"@\", encoding=\"iso-8859-1\", header=None)\n",
    "    df = df.rename(columns={0: 'Sentences', 1: 'Sentiment'})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_file_data(sentences_All)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanatory Analysis**\n",
    "1. **Sentiment Tagger** - 4 csv files containing finance headlines and sentiment labels (Positive, Neutral and Negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list to collect data\n",
    "data = []\n",
    "\n",
    "# Reading files\n",
    "for file in files:\n",
    "\n",
    "    # Read file data\n",
    "    df = get_file_data(file)\n",
    "    \n",
    "    # Collect data for each file\n",
    "    file_data = {\n",
    "        'File': file.split('/')[-1],\n",
    "        'Number of NaN values': df.isna().sum().sum(),  # Total NaN values across all columns\n",
    "        'Number of records': df['Sentiment'].count()\n",
    "    }\n",
    "    \n",
    "    # Number of occurrences for each sentiment\n",
    "    sentiment_counts = df['Sentiment'].value_counts().to_dict()\n",
    "    for sentiment, count in sentiment_counts.items():\n",
    "        file_data[f'Occurrences - {sentiment}'] = count\n",
    "    \n",
    "    # Percentage of occurrences for each sentiment\n",
    "    sentiment_percentages = df['Sentiment'].value_counts(normalize=True).to_dict()\n",
    "    for sentiment, percentage in sentiment_percentages.items():\n",
    "        file_data[f'Percentage - {sentiment}'] = round(percentage * 100, 2)\n",
    "    \n",
    "    # Append the data for this file to the list\n",
    "    data.append(file_data)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "summary_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot style\n",
    "sns.set_theme()  # Applies the default Seaborn theme\n",
    "sns.set_palette('pastel')  # Set the color palette\n",
    "\n",
    "# Plot histograms for the 'Values' column in each DataFrame\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "for i, file in enumerate(files):\n",
    "    \n",
    "    # Read file data\n",
    "    df = get_file_data(file)\n",
    "\n",
    "    # Find the index of the sequence in the sentence\n",
    "    start_index = file.find(\"Sentences\")\n",
    "    end_index = file.find(\".txt\")\n",
    "    \n",
    "    if start_index != -1:  # If the sequence is found in the sentence\n",
    "        # Extract the substring from the index of the sequence\n",
    "        extracted_title = file[start_index:end_index]\n",
    "\n",
    "    ax[i//2, i%2].bar(df['Sentiment'].value_counts().index, df['Sentiment'].value_counts(), color=['grey', 'green', 'red'])\n",
    "    ax[i//2, i%2].set_title(extracted_title)\n",
    "    ax[i//2, i%2].set_xlabel('Sentiment')\n",
    "    ax[i//2, i%2].set_ylabel('Count')\n",
    "\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the preprocessing step**, we will use the module CountVectorizer() from the python-library scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_df(df):\n",
    "    \"\"\" Vectorizer function \"\"\"\n",
    "    # Vectorizing the sentences\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    X = vectorizer.fit_transform(df.iloc[:,0])\n",
    "    y = df.iloc[:,1]\n",
    "\n",
    "    return X, y, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 2: Train and fine-tune various NLP classifiers on financial news datasets** \n",
    "\n",
    "#### **2.1 Let´s start with simple baseline (at your own choice)**. For example, build a logistic regression model based on pre-trained word embeddings or TF-IDF vectors of the financial news corpus.\n",
    "\n",
    "\n",
    "Build a baseline model  with **Financial Phrasebank dataset**. What are the limitations of these baseline models?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the financial sentence analysis plots, it's evident that we're dealing with an imbalanced dataset, where the `neutral` category dominates over `positive` and `negative` categories. To address this issue, and considering the complexities involved in gathering more data, or resampling, we'll adopt a simpler approach for our logistic regression base model. Specifically, we'll employ class weighting to rebalance the classes, using the `class_weight` parameter. This allows us to assign weights inversely proportional to class frequencies.\n",
    "\n",
    "Moreover, beyond solely relying on accuracy, we'll explore additional metrics such as F1 score, precision-recall curves, AUC-ROC curves, and confusion matrices. These metrics offer a more comprehensive understanding of the model's performance, enabling us to make more informed decisions about its efficacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline_model(X_train, y_train):\n",
    "    \"\"\" Model creation fucntion (logistic regression)\"\"\"\n",
    "    \n",
    "    model = LogisticRegression(class_weight='balanced')\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return model    \n",
    "\n",
    "def predict_baseline_model(model, X_test, y_test):\n",
    "    \"\"\" Model prediction function\"\"\"\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Compute metrics\n",
    "    class_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    return y_pred, class_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the results\n",
    "results = []\n",
    "# Conncatenate dataframes\n",
    "dfs = []\n",
    "\n",
    "for file in files:\n",
    "    \n",
    "    file_name = file.split('/')[-1] # File namings\n",
    "    df = get_file_data(file) # Read file data\n",
    "    dfs.append(df) # For dataframe conncatenation\n",
    "    X, y, vectorizer = vectorize_df(df) # Vectorize data\n",
    "\n",
    "    # Train / Test splits\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "    # Baseline model and predictions\n",
    "    bl_model = create_baseline_model(X_train, y_train)\n",
    "    bl_model_pred, class_report = predict_baseline_model(bl_model, X_test, y_test)\n",
    "\n",
    "    # Remove the 'accuracy' entry if it exists\n",
    "    class_report.pop('accuracy', None)\n",
    "    \n",
    "    # Append the classification report to the results list\n",
    "    for label, metrics in class_report.items():\n",
    "        metrics['File'] = file_name\n",
    "        metrics['Label'] = label\n",
    "        results.append(metrics)\n",
    "\n",
    "    # Save the vectorizer\n",
    "    vects_path = f\"../vectorizers/vectorizer_{file_name[10:-4]}.pkl\"\n",
    "    with open(vects_path, 'wb') as vect_file:\n",
    "        pickle.dump(vectorizer, vect_file)\n",
    "    \n",
    "    models_path = f\"../models/log-reg_mod_{file_name[10:-4]}.pkl\"\n",
    "    # Save the logistic regression model\n",
    "    with open(models_path, 'wb') as mod_file:\n",
    "        pickle.dump(bl_model, mod_file)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "results_df = pd.DataFrame(results).set_index(['File', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the model with new data, we can use the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(input_text, vectorizer, model):\n",
    "    \"\"\"\n",
    "    Predicts the sentiment of the input text using the provided vectorizer and logistic regression model.\n",
    "\n",
    "    Parameters:\n",
    "        input_text: str, the text for which the sentiment needs to be predicted.\n",
    "        vectorizer: CountVectorizer, the vectorizer that was fitted on the training data.\n",
    "        model: LogisticRegression, the trained logistic regression model.\n",
    "\n",
    "    Returns:\n",
    "        str, the predicted sentiment label for the input text.\n",
    "    \"\"\"\n",
    "    # Vectorize the input text using the fitted CountVectorizer\n",
    "    input_text_vectorized = vectorizer.transform(input_text)\n",
    "\n",
    "    # Predict the sentiment using the trained Logistic Regression model\n",
    "    sentiment_prediction = model.predict(input_text_vectorized)\n",
    "\n",
    "    return sentiment_prediction[0]  # Return the predicted sentiment label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New text example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"For the last quarter of 2010 , Componenta 's net sales drop to EUR76m from EUR167m for the same period a year earlier\"\n",
    "\n",
    "# Load model\n",
    "model_path = \"../models/log-reg_mod_AllAgree.pkl\"\n",
    "with open(model_path, 'rb') as mod_file:\n",
    "    load_model = pickle.load(mod_file)\n",
    "\n",
    "# Load vectorizer\n",
    "vectorizer_path = \"../vectorizers/vectorizer_AllAgree.pkl\"\n",
    "with open(vectorizer_path, 'rb') as vec_file:\n",
    "    load_vectorizer = pickle.load(vec_file)\n",
    "\n",
    "# Transform the new text using the loaded vectorizer\n",
    "new_text_vectorized = load_vectorizer.transform([input_text])\n",
    "\n",
    "# Predict the sentiment using the loaded model\n",
    "predicted_sentiment = load_model.predict(new_text_vectorized)\n",
    "\n",
    "print(f\"Predicted Sentiment: {predicted_sentiment[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's apparent that the model is struggling to perform well with new data. This could be attributed primarily to either class imbalance or shortcomings in the training procedure. Moreover, it's worth noting that employing a model specifically tailored for sentences might not be ideal for handling the characteristics of the encountered new text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T09:28:22.708863Z",
     "start_time": "2024-02-18T09:28:22.694494Z"
    }
   },
   "source": [
    "#### **2.2 Compare the baseline with a pre-trained model that is specialized for the finance domain. Download and use the FinBERT model from Huggingfaces**\n",
    "\n",
    "Model source: https://huggingface.co/ProsusAI/finbert\n",
    "\n",
    "Once you have downloaded the model, run inference and compute performance metrics to get a sense of how the specialized pre-trained model fares against the baseline  model.  Use the HuggingFaces library to download the model and run inference on it. For large datasets or text sequences, CPU running time might be large.\n",
    "\n",
    "For more information on the model: Araci, D. (2019). FinBERT: Financial Sentiment Analysis with Pre-trained Language Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "model_name = \"ProsusAI/finbert\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def sentiment_prediction(text, tokenizer, model):\n",
    "    \"\"\" Sentiment prediction with BERT model\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    return torch.argmax(probs, dim=1).numpy()\n",
    "\n",
    "# Mapping for evaluation\n",
    "label_map = {0: 'positive', 1: 'negative', 2: 'neutral'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the BERT vectorizer\n",
    "vects_path = \"../vectorizers/small_FinBERT_vectorizer.pkl\"\n",
    "with open(vects_path, 'wb') as vect_file:\n",
    "    pickle.dump(vectorizer, vect_file)\n",
    "    \n",
    "models_path = \"../models/small_FinBERT_model.pkl\"\n",
    "# Save the BERT model\n",
    "with open(models_path, 'wb') as mod_file:\n",
    "    pickle.dump(model, mod_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following step, we will try the BERT model with the same data as the logistic regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences_df = get_file_data(sentences_All)\n",
    "\n",
    "# Features (sentences) and target (Sentiment)\n",
    "X = all_sentences_df['Sentences']\n",
    "y = all_sentences_df['Sentiment']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict sentiments for the training set\n",
    "train_preds = [sentiment_prediction(text, tokenizer, model)[0] for text in X_train]\n",
    "\n",
    "# Predict sentiments for the test set\n",
    "test_preds = [sentiment_prediction(text, tokenizer, model)[0] for text in X_test]\n",
    "\n",
    "# Apply the prediction function to text data\n",
    "all_sentences_df['Predicted_Sentiment'] = all_sentences_df['Sentences'].apply(lambda x: sentiment_prediction([x], tokenizer, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions to their corresponding sentiment labels\n",
    "train_preds_labels = [label_map[pred] for pred in train_preds]\n",
    "test_preds_labels = [label_map[pred] for pred in test_preds]\n",
    "\n",
    "# Evaluate model performance on the training and test sets\n",
    "print(\"Training Set Performance:\")\n",
    "print(classification_report(y_train, train_preds_labels))\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(classification_report(y_test, test_preds_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, we're conducting tests on a single dataframe. To broaden our analysis, we'll consolidate all dataframes using the variable `dfs` established earlier. After concatenating them, we'll address any duplicate entries and then iterate through the procedure once more to facilitate a comparative assessment of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all dataframes into one\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Check for duplicated sentences\n",
    "duplicates = combined_df.duplicated(subset=['Sentences'])\n",
    "\n",
    "# Optional: Display duplicated sentences\n",
    "print(combined_df[duplicates])\n",
    "\n",
    "# Drop duplicates if needed\n",
    "combined_df.drop_duplicates(subset=['Sentences'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (sentences) and target (Sentiment)\n",
    "X = combined_df['Sentences']\n",
    "y = combined_df['Sentiment']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict sentiments for the training set\n",
    "train_preds = [sentiment_prediction(text, tokenizer, model)[0] for text in X_train]\n",
    "\n",
    "# Predict sentiments for the test set\n",
    "test_preds = [sentiment_prediction(text, tokenizer, model)[0] for text in X_test]\n",
    "\n",
    "# Apply the prediction function to text data\n",
    "all_sentences_df['Predicted_Sentiment'] = all_sentences_df['Sentences'].apply(lambda x: sentiment_prediction([x], tokenizer, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions to their corresponding sentiment labels\n",
    "train_preds_labels = [label_map[pred] for pred in train_preds]\n",
    "test_preds_labels = [label_map[pred] for pred in test_preds]\n",
    "\n",
    "# Evaluate model performance on the training and test sets\n",
    "print(\"Training Set Performance:\")\n",
    "print(classification_report(y_train, train_preds_labels))\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(classification_report(y_test, test_preds_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now, we've utilized the BERT base model, comprising 12 layers of Transformer encoder, 12 attention heads, a hidden size of 768, and 110M parameters. Moving forward, we'll transition to employing the BERT large model, characterized by 24 layers of Transformer encoder, 16 attention heads, a hidden size of 1024, and 340 parameters.\n",
    "\n",
    "This shift is motivated by our forthcoming utilization of extensive text data from {REDDIT - WHATEVER} to analyze the sentiment of financial text related to {BITCON - WHATEVER}. Given the complexity of this task, necessitating extensive NLP processing, we aim to fine-tune our approach. Here's how we plan to implement it:\n",
    "\n",
    "- `[CLS]`: Serving as the initial token in every sequence, this denotes the classification token.\n",
    "\n",
    "    For classification tasks, we must prepend the special `[CLS]` token to the beginning of every sentence.\n",
    "\n",
    "    This token has special significance. BERT consists of 12 Transformer layers. Each transformer takes in a list of token embeddings, and produces the same number of embeddings on the output (but with the feature values changed, of course!).\n",
    "\n",
    "- `[SEP]`: This special token demarcates to BERT which tokens belong to distinct sequences. While crucial for tasks like next sentence prediction or question-answering, if we're dealing with a single sequence, it's appended to its end.\n",
    "\n",
    "    At the end of every sentence, we need to append the special `[SEP]` token. \n",
    "\n",
    "    This token is an artifact of two-sentence tasks, where BERT is given two separate sentences and asked to determine something (e.g., can the answer to the question in sentence A be found in sentence B?). \n",
    "\n",
    "BERT has two constraints:\n",
    "\n",
    "- All sentences must be padded or truncated to a single, fixed length.\n",
    "- The maximum sentence length is 512 tokens.\n",
    "  \n",
    "    It's worth noting that the maximum token input for BERT models is 512. If a sequence falls short, we can pad the unused slots with [PAD] tokens. Conversely, if a sequence exceeds 512 tokens, truncation is necessary.\n",
    "\n",
    "    That's the entirety of what BERT expects for input. The BERT model then yields embedding vectors of size 768 for each token. These vectors serve as versatile inputs for various NLP applications, spanning text classification, next sentence prediction, Named-Entity-Recognition (NER), or question-answering tasks.\n",
    "\n",
    "Padding is done with a special `[PAD]` token, which is at index 0 in the BERT vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer and model with a pre-trained BERT model specifically fine-tuned for financial texts\n",
    "tokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "# Save the tokenizer\n",
    "tok_path = \"../tokenizers/large_FinBERT_tokenizer.pkl\"\n",
    "with open(tok_path, 'wb') as tok_file:\n",
    "    pickle.dump(tokenizer, tok_file)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('ProsusAI/finbert')\n",
    "# Save the model\n",
    "vects_path = \"../models/large_FinBERT_model.pkl\"\n",
    "with open(vects_path, 'wb') as vect_file:\n",
    "    pickle.dump(vectorizer, vect_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text to be tokenized and analyzed\n",
    "txt = \"\"\"\n",
    "1. HIGHER LEVELS OF RETAIL PARTICIPATION IN CRYPTO THAN TRADITIONAL COMMODITY MARKETS POSE UNIQUE CHALLENGES FOR REGULATORS.\n",
    "\n",
    "One in five Americans report having traded cryptocurrency, and polls suggest crypto trading is more common among younger adults, men, and racial minorities. This is quite different from other financial instruments regulated by the CFTC, Benham noted. “You’re going to have more vulnerable investors… It’s incumbent on us to educate, to inform, to disclose risks involved.”\n",
    "\n",
    "Michael Piwowar, a former Securities and Exchange Commissioner and now executive director of the Milken Institute Center for Financial Markets, linked increased Congressional attention to growth in retail crypto: “If you got one in five households that have interacted with crypto… [members of Congress] are going to start hearing it from their constituents.” Legislation to regulate digital assets has been introduced by Senators Lummis and Gillibrand, Stabenow and Boozman, and Toomey, as well as Representative Gottheimer. The Treasury is actively negotiating bipartisan stablecoin legislation with House Financial Services Committee Chair Waters and Ranking Member McHenry. Benham said that stablecoins, digital currency meant to always be equal to one dollar, are more of a “payment mechanism” and thus should be regulated by prudential banking regulators.\n",
    "\n",
    "Digital asset regulation may require addressing crypto exchanges and digital wallets. American University Law Professor Hilary Allen noted that the stablecoin legislation under discussion does not, saying, “That is a gaping hole… Almost every major stablecoin… is affiliated with an exchange that profits from trading in that stablecoin.” Mark Wetjen, a former CFTC commissioner and current head of policy and regulatory strategy for FTX (one of the largest crypto exchanges), agreed: “The exchanges are the gateways to the entire crypto space, and so oversight of them is probably most important.” He pushed back that there was no current regulation, noting the requirement for state level licenses, such as New York’s Bitlicense: “If you want to list derivatives on bitcoin, for example, you need a license… so it may not be as dire a situation.”\n",
    "\n",
    "2. CRYPTO CHALLENGES TRADITIONAL REGULATORY DISTINCTION BETWEEN SECURITIES AND COMMODITIES.\n",
    "\n",
    "Traditionally, the SEC regulates securities while the CFTC regulates commodities and derivatives. Whether crypto is a security or commodity remains unclear, as various subcomponents of the crypto ecosystem challenge existing regulatory divisions. For instance, the SEC recently argued  that nine different crypto tokens were securities in an insider trading case while a federal judge ruled that virtual currency like Bitcoin constitutes a commodity.\n",
    "\n",
    "Benham called on Congress to provide clarity on which of the hundreds – if not thousands – of coins in existence are securities versus commodities: “Ultimately, we’d like to see law drawing lines.” Piwowar said the lack of clarity creates unwelcome delays as many crypto-related applications before the SEC are “not getting answers” on whether their products represent securities. The result is that some crypto firms are “going outside the United States” to locate their business. Allen cautioned, though, that Congressional action could also constitute an indication that the government supports crypto. She warned against letting crypto into the regulated sphere for fear of giving it “implicit guarantees.”\n",
    "\n",
    "A solution to the regulatory turf battle could be merging the SEC and CFTC, which Piwowar endorsed, as have many others. Congress, however, has shown little appetite to do so given the different Congressional committee jurisdictions involved.\n",
    "\n",
    "3. CFTC WILL RESTRUCTURE TO BETTER PROTECT CONSUMERS AND MORE EFFECTIVELY REGULATE MARKETS.\n",
    "Benham announced several changes at the CFTC during the Brookings event. First, LabCFTC will become the Office of Technology Innovation, reporting directly to the Chairman’s office. Behnam justified this by stating, “We are past the incubator stage, and digital assets and decentralized financial technologies have outgrown their sandboxes.” Second, CFTC’s Office of Customer Education and Outreach will be realigned within the Office of Public Affairs, which Behnam said would “leverage resources and a broader understanding of the issues facing the general public towards addressing the most critical needs in the most vulnerable communities.” Restructuring within a regulator may appear a bureaucratic shuffle but can reflect changes in internal power, agency focus, and prioritization. Directly reporting to the chair increases an office’s authority and prestige.\n",
    "\n",
    "4. IS CRYPTO A PASSING FAD (OR WORSE, A BUBBLE THAT THREATENS FINANCIAL MARKETS)?\n",
    "Allen argued that crypto is “purposely less efficient and more complicated than a more centralized system,” and does not have any societal value. FTX’s Wetjen disagreed: “The difference here with blockchain as the underpinning means by which you can transfer value is that there are absolutely no gates.” Piwowar broadly agreed with Wetjen that “We’re going to have the new generation of Amazons and Googles come out of this stuff,” but cautioned that while he was at the SEC, “Nine out of ten [crypto applications] were outright fraud, and then out of the one out of ten, nine out of ten of those were probably fraud.” Since January 2021, over 46,000 people have collectively lost over $1 billion to scams involving crypto.\n",
    "\n",
    "Everyone wants to avoid a repeat of the 2008 global financial crisis. To do so, regulators have focused on avoiding and mitigating “systemic risk” to the financial system. Asked if he sees a “clear and present danger to the existing economic system,” Benham said he did not, pointing out that crypto is not sufficiently interconnected to pose systemic risk. He noted the decrease in crypto values over the past several months did not cause ripples in the financial system or the broader economy. Piwowar turned the question of systemic risk back onto the actions of financial regulators asking: “What is systemic risk?  It’s the risk that a federal policymaker is going to bail out a bank, either directly or indirectly.” Allen agreed that bailing out crypto would be a mistake quipping: “If anything should be able to fail, it should be crypto, which isn’t… funding productive economic capacity.”\n",
    "\n",
    "Allen also noted the similarity in arguments centered on American global competitiveness which promoted lax regulation for derivatives: “It’s almost identical to the rhetoric we saw around swaps in the 1990s.” Credit default swaps, like crypto now, faced loose regulation and ultimately helped fuel the subprime mortgage crisis. Behnam noted that one of 2008’s biggest lessons was the need for the CFTC to promote market transparency in the “OTC [over-the-counter] derivative space.” Crypto proponents point to the underlying technology as being inherently more transparent, while critics point to the lack of understanding of aspects of the market, such as what assets back stablecoins like Tether.\n",
    "\n",
    "5. DOES CRYPTO INCREASE FINANCIAL INCLUSION?\n",
    "Cryptocurrency proponents frequently cite financial inclusion as a major benefit linking the higher usage of youth and communities of color who have higher rates of being unbanked or underbanked by traditional finance. Allen cautioned against “predatory inclusion” arguing that, “Because there’s no productive capacity behind them, their value derives from finding someone else to buy them from you.” Wetjen’s responded, blending his experience serving as a CFTC Commissioner with his time in the crypto industry: “From my own experience… at the CFTC, there’s plenty of authority that’s already in place for the agency to… be pretty thoughtful and relatively prescriptive, even in terms of what actually should be disclosed to, particularly, retail investors, or users of a platform such as FTX.” He argued that the right policy is “giving people the opportunity to be involved and invest in the space that they like but making sure that it’s done with the right safeguards.”\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we'll undertake the required preprocessing to ensure that the model's input accepts a tensor-shaped format rather than a list of elements, as the latter could lead to errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text, specifying not to add special tokens and to return PyTorch tensors\n",
    "tokens = tokenizer.encode_plus(txt, add_special_tokens = False, return_tensors = 'pt')\n",
    "\n",
    "# Print the number of keys in the tokenized output (input_ids, attention_mask, etc.)\n",
    "print(tokens)\n",
    "print(f\"Number of keys in token dictionary: {len(tokens)}\")\n",
    "print(f\"Sequence lenght: {len(tokens['input_ids'][0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we surpass the maximum sequence length for this model, we'll need to adopt the sliding window approach to accommodate the token length. For this, we will use the Pytorch capabilities to split in chunks of windows of 512.\n",
    "\n",
    "Additionally, in this section we will be using the \"Attention Mask\" concept.\n",
    "\n",
    "The \"Attention Mask\" is simply an array of 1s and 0s indicating which tokens are padding and which aren't (seems kind of redundant, doesn't it?!). This mask tells the \"Self-Attention\" mechanism in BERT not to incorporate these PAD tokens into its interpretation of the sentence.\n",
    "\n",
    "The maximum length does impact training and evaluation speed, however. \n",
    "For example, with a Tesla K80 (GPU):\n",
    "\n",
    "`MAX_LEN = 128  -->  Training epochs take ~5:28 each`\n",
    "\n",
    "`MAX_LEN = 64   -->  Training epochs take ~2:57 each`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the tokenized input_ids and attention_mask into chunks of size 510\n",
    "input_id_chunks = tokens['input_ids'][0].split(510)\n",
    "attention_mask_chunks = tokens['attention_mask'][0].split(510)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking on the lengths and shapes for inputs and chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the length of the chunks list for both input_ids and attention_mask\n",
    "print(f\"Number of chunks: {len(attention_mask_chunks)}\")\n",
    "print(f\"Size of the first chunk: {len(attention_mask_chunks[0])}\")\n",
    "\n",
    "# Check the shape of the first chunk of input_ids\n",
    "print(f\"Shape of the first input_id chunk {input_id_chunks[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we are ready to encode our text, though, we need to decide on a maximum sentence length for padding / truncating to. In the following section, we will achieve this and go through the following steps my making use of the Pytorch capabilities:\n",
    "\n",
    "    (1) Tokenize the sentence.\n",
    "    (2) Prepend the `[CLS]` token to the start.\n",
    "    (3) Append the `[SEP]` token to the end.\n",
    "    (4) Map tokens to their IDs.\n",
    "    (5) Pad or truncate the sentence to `max_length`\n",
    "    (6) Create attention masks for [PAD] tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_ids_and_attention_mask_chunk():\n",
    "    \"\"\"\n",
    "    Splits the input_ids and attention_mask tensors into chunks, ensuring that each chunk, except possibly the last,\n",
    "    has a length of `chunksize - 2` to accommodate for the addition of special tokens. Special tokens ([CLS] and [SEP])\n",
    "    are prepended and appended to each chunk respectively. Chunks are padded with zeros to maintain uniform size.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing two lists of tensors, one for the chunked input_ids and the other for the chunked attention masks.\n",
    "    \"\"\"\n",
    "    chunksize = 512\n",
    "    # Split the tokens into chunks, leaving space for special tokens\n",
    "    input_id_chunks = list(tokens['input_ids'][0].split(chunksize - 2))\n",
    "    attention_mask_chunks = list(tokens['attention_mask'][0].split(chunksize - 2))\n",
    "    \n",
    "    # Iterate through each chunk to add special tokens and padding\n",
    "    for i in range(len(input_id_chunks)):\n",
    "        # Prepend [CLS] and append [SEP] tokens (101 and 102 respectively)\n",
    "        input_id_chunks[i] = torch.cat([\n",
    "            torch.tensor([101]), input_id_chunks[i], torch.tensor([102])\n",
    "        ])\n",
    "        # Prepend and append attention mask bits for special tokens\n",
    "        attention_mask_chunks[i] = torch.cat([\n",
    "            torch.tensor([1]), attention_mask_chunks[i], torch.tensor([1])\n",
    "        ])\n",
    "\n",
    "        # Calculate padding length\n",
    "        pad_length = chunksize - input_id_chunks[i].shape[0]\n",
    "        # Apply padding if necessary\n",
    "        if pad_length > 0:\n",
    "            input_id_chunks[i] = torch.cat([\n",
    "                input_id_chunks[i], torch.Tensor([0] * pad_length)\n",
    "            ])\n",
    "            attention_mask_chunks[i] = torch.cat([\n",
    "                attention_mask_chunks[i], torch.Tensor([0] * pad_length)\n",
    "            ])\n",
    "            \n",
    "    return input_id_chunks, attention_mask_chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the processed chunks for input_ids and attention_mask\n",
    "input_id_chunks, attention_mask_chunks = get_input_ids_and_attention_mask_chunk()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we'll combine the individual tensors for `input_ids` and `attention_mask`, stacking them to consolidate all tensor information into a single `input_dict` variable. This resulting tensor will serve as the input for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the chunks to form tensors\n",
    "input_ids = torch.stack(input_id_chunks)\n",
    "attention_mask = torch.stack(attention_mask_chunks)\n",
    "\n",
    "# Prepare the input dictionary for the model\n",
    "input_dict = {\n",
    "    'input_ids' : input_ids.long(),\n",
    "    'attention_mask' : attention_mask.int()\n",
    "}\n",
    "\n",
    "print(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shapes of the input tensors\n",
    "print(input_dict['input_ids'].shape)\n",
    "print(input_dict['attention_mask'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful, with the shape [4, 512], we can confidently affirm its compatibility with FinBert's requirements, considering the maximum token length (sequence length) of 512 and the specified number of batches, which is 4 in this scenario. Having verified this crucial step, we can proceed by invoking the model to obtain prediction outputs in the form of class probabilities using softmax layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the inputs through the model to get the outputs\n",
    "outputs = model(**input_dict)\n",
    "\n",
    "# Apply softmax to the logits to get probabilities\n",
    "probabilities = torch.nn.functional.softmax(outputs[0], dim=-1)\n",
    "\n",
    "# Compute the mean probabilities across all chunks\n",
    "mean_probabilities = probabilities.mean(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, we'll map these probabilities to the respective \"Positive\", \"Neutral\", and \"Negative\" classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the mean probabilities and the most likely class\n",
    "print(f\"Mean probabilities: {mean_probabilities}\")\n",
    "\n",
    "# Map the index to its corresponding label\n",
    "predicted_class_index = torch.argmax(mean_probabilities).item()\n",
    "print(f\"Index of the highest probability class: {predicted_class_index}\")\n",
    "predicted_class_label = label_map[predicted_class_index]\n",
    "\n",
    "# Print the predicted class label\n",
    "print(f\"Predicted class: {predicted_class_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.3 (Advanced) Fine-tune a pre-trained model such a base BERT model on a small labeled dataset**\n",
    "\n",
    "General-purpose models are not effective enough because of the specialized language used in a financial context. We hypothesize that pre-trained language models can help with this problem because they require fewer labeled examples and they can be further trained on domain-specific corpora.\n",
    "\n",
    "In recent years the NLP community has seen many breakthoughs in Natural Language Processing, especially the shift to transfer learning. Models like ELMo, fast.ai's ULMFiT, Transformer and OpenAI's GPT have allowed researchers to achieves state-of-the-art results on multiple benchmarks and provided the community with large pre-trained models with high performance. This shift in NLP is seen as NLP's ImageNet moment, a shift in computer vision a few year ago when lower layers of deep learning networks with million of parameters trained on a specific task can be reused and fine-tuned for other tasks, rather than training new networks from scratch.\n",
    "\n",
    "One of the most significant milestones in the evolution of NLP recently is the release of Google's BERT, which is described as the beginning of a new era in NLP. In our case, we are going to explore a pre-trained model called FinBERT, already tuned with a financial corpus. I specifically recommend the HuggingFace library for easeness of implementation.\n",
    "\n",
    "*What is HuggingFace?* Hugging Face’s Transformers is an open-source library that provides thousands of pre-trained models to perform various tasks on texts such as text classification, named entity recognition, translation, and more. The library has a unified, high-level API for these models and supports a wide range of languages and model architectures.\n",
    "\n",
    "\n",
    "Here are various tutorials for finetuning BERT: https://drlee.io/fine-tuning-hugging-faces-bert-transformer-for-sentiment-analysis-69b976e6ac5d and https://skimai.com/fine-tuning-bert-for-sentiment-analysis/. I specially recommnend this one: http://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
    "\n",
    "The dataset where to finetune a BERT related model can be found in the previous cell: **Financial tweets topics dataset** \n",
    "\n",
    "*ALERT*: Running or training a large language model like BERT or FinBERT might incur in large CPU processing times. Although BERT is very large, complicated, and have millions of parameters, we might only need to fine-tune it in only 2-4 epochs. You can also explore Google colab, for limited acces to free GPUs, which might best suited for this task., specially if training required.\n",
    "\n",
    "Finally, compare the previous baseline with fine-tuned FinBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the progress made thus far, for the fine tuning setp, we've opted to utilize a dataset sourced from the Hugging Face repositories. \n",
    "\n",
    "This dataset pertains to cryptocurrencies and Bitcoin, encompassing comments categorized as Positive, Negative, and Neutral. It can be accessed [here](https://huggingface.co/datasets/Andyrasika/alpaca-bitcoin-sentiment-dataset/viewer/default/train?p=1). Given the observed class imbalance within this dataset, we have chosen to supplement it by exclusively incorporating rows labeled as \"Negative\" from another dataset, available [here](https://huggingface.co/datasets/ckandemir/bitcoin_tweets_sentiment_kaggle/viewer/default/train?p=1).\n",
    "\n",
    "Our focus during labeling has been to prioritize English sentences so the model can be optimized in this language with crypto currency and bitcoin argon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Using Colab GPU for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this fine-tuning phase, utilizing a GPU for training is highly recommended. Google Colab notebooks provide an excellent option as they offer free access to GPUs and TPUs. Given that we'll be training a large neural network, leveraging this resource is essential to expedite the process; otherwise, training duration could be significantly prolonged.\n",
    "\n",
    "To attach a GPU, navigate to the menu and select:\n",
    "\n",
    "`Edit 🡒 Notebook Settings 🡒 Hardware accelerator 🡒 (GPU)`\n",
    "\n",
    "Afterward, execute the following cell to verify GPU detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading Alpaca Bitcoin Sentiment Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As outr development, we will use and tune the model we have been using `BertForSequenceClassification.from_pretrained('ProsusAI/finbert')` with the enhanced [Andyrasika/alpaca-bitcoin-sentiment-dataset](https://huggingface.co/datasets/Andyrasika/alpaca-bitcoin-sentiment-dataset/viewer/default/train?f[output][value]=%27Neutral%27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The merged dataset can be accesible in this project's repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-05-16</td>\n",
       "      <td>@Quark_Chain #QKC #Blockchain #BTC #ETH #block...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-07-11</td>\n",
       "      <td>As of July 10, 2019 at 09:50AM, 1 BTC = 11671....</td>\n",
       "      <td>Negative</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>https://t.co/utvdy6QR4r\\nClick the link\\nSign ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-17</td>\n",
       "      <td>Bitcoin Price Correction Begins After a Massiv...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-12-18</td>\n",
       "      <td>#EuroCoin #EUC $0.000458 (44.80%) 0.00000058 B...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                                               text Sentiment  \\\n",
       "0  2019-05-16  @Quark_Chain #QKC #Blockchain #BTC #ETH #block...  Negative   \n",
       "1  2019-07-11  As of July 10, 2019 at 09:50AM, 1 BTC = 11671....  Negative   \n",
       "2  2019-07-01  https://t.co/utvdy6QR4r\\nClick the link\\nSign ...  Negative   \n",
       "3  2019-05-17  Bitcoin Price Correction Begins After a Massiv...  Negative   \n",
       "4  2016-12-18  #EuroCoin #EUC $0.000458 (44.80%) 0.00000058 B...  Negative   \n",
       "\n",
       "  language  \n",
       "0       en  \n",
       "1       en  \n",
       "2       en  \n",
       "3       en  \n",
       "4       en  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merged CSV file URL\n",
    "csv_url = \"https://raw.githubusercontent.com/pablo-git8/FinSentNewsNLP/main/data/raw/merged-bitcoin-sentiment.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_url)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define the sentences and labels variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting the mapping\n",
    "inv_label_map = {'Positive': 0, 'Negative': 1, 'Neutral': 2}\n",
    "\n",
    "# Get the lists of sentences and their labels.\n",
    "sentences = df.text.values\n",
    "labels = np.array([inv_label_map[label] for label in df.Sentiment.values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying label format for tensor conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenization and Input Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll transform our dataset into the format that BERT can be trained on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer and model with a pre-trained BERT model specifically fine-tuned for financial texts\n",
    "print(\"Load the BERT tokenizer...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing a random sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  @Quark_Chain #QKC #Blockchain #BTC #ETH #blockchaintechnology #sharding\n",
      "ыафвыв\n",
      "ыв\n",
      "аы\n",
      "Tokenized:  ['@', 'qu', '##ark', '_', 'chain', '#', 'q', '##k', '##c', '#', 'block', '##chai', '##n', '#', 'bt', '##c', '#', 'et', '##h', '#', 'block', '##chai', '##nte', '##ch', '##nology', '#', 'sha', '##rdi', '##ng', 'ы', '##а', '##ф', '##в', '##ы', '##в', 'ы', '##в', 'а', '##ы']\n",
      "Token IDs:  [1030, 24209, 17007, 1035, 4677, 1001, 1053, 2243, 2278, 1001, 3796, 24925, 2078, 1001, 18411, 2278, 1001, 3802, 2232, 1001, 3796, 24925, 10111, 2818, 21020, 1001, 21146, 17080, 3070, 1206, 10260, 29749, 25529, 29113, 25529, 1206, 25529, 1180, 29113]\n"
     ]
    }
   ],
   "source": [
    "# Print the original sentence.\n",
    "print(' Original: ', sentences[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  163\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\pablo\\.conda\\envs\\finsent-env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2619: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  @Quark_Chain #QKC #Blockchain #BTC #ETH #blockchaintechnology #sharding\n",
      "ыафвыв\n",
      "ыв\n",
      "аы\n",
      "Token IDs: tensor([  101,  1030, 24209, 17007,  1035,  4677,  1001,  1053,  2243,  2278,\n",
      "         1001,  3796, 24925,  2078,  1001, 18411,  2278,  1001,  3802,  2232,\n",
      "         1001,  3796, 24925, 10111,  2818, 21020,  1001, 21146, 17080,  3070,\n",
      "         1206, 10260, 29749, 25529, 29113, 25529,  1206, 25529,  1180, 29113,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Training & Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our input data is properly formatted, it's time to fine tune the BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,986 training samples\n",
      "  221 validation samples\n"
     ]
    }
   ],
   "source": [
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also create an iterator for our dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Our Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our input data is properly formatted, it's time to fine tune the BERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task. \n",
    "\n",
    "Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.  \n",
    "\n",
    "Here is the current list of classes provided for fine-tuning:\n",
    "* BertModel\n",
    "* BertForPreTraining\n",
    "* BertForMaskedLM\n",
    "* BertForNextSentencePrediction\n",
    "* **BertForSequenceClassification** - The one we'll use.\n",
    "* BertForTokenClassification\n",
    "* BertForQuestionAnswering\n",
    "\n",
    "The documentation for these can be found under [here](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using [BertForSequenceClassification](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#bertforsequenceclassification). This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's load BERT! There are a few different pre-trained BERT models available. \"bert-base-uncased\" means the version that has only lowercase letters (\"uncased\") and is the smaller version of the two (\"base\" vs \"large\").\n",
    "\n",
    "The documentation for `from_pretrained` can be found [here](https://huggingface.co/transformers/v2.2.0/main_classes/model.html#transformers.PreTrainedModel.from_pretrained), with the additional parameters defined [here](https://huggingface.co/transformers/v2.2.0/main_classes/configuration.html#transformers.PretrainedConfig)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"ProsusAI/finbert\", # Use the 12-layer BERT model, with an ProsusAI/finbert vocab.\n",
    "    num_labels = 3, # The number of output labels--3 for multi classification. \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "#model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for curiosity's sake, we can browse all of the model's parameters by name here.\n",
    "\n",
    "In the below cell, I've printed out the names and dimensions of the weights for:\n",
    "\n",
    "1. The embedding layer.\n",
    "2. The first of the twelve transformers.\n",
    "3. The output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (3, 768)\n",
      "classifier.bias                                                 (3,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Optimizer & Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model loaded we need to grab the training hyperparameters from within the stored model.\n",
    "\n",
    "For the purposes of fine-tuning, the authors recommend choosing from the following values (from Appendix A.3 of the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf)):\n",
    "\n",
    ">- **Batch size:** 16, 32  \n",
    "- **Learning rate (Adam):** 5e-5, 3e-5, 2e-5  \n",
    "- **Number of epochs:** 2, 3, 4 \n",
    "\n",
    "We chose:\n",
    "* Batch size: 32 (set when creating our DataLoaders)\n",
    "* Learning rate: 2e-5\n",
    "* Epochs: 4 (we'll see that this is probably too many...)\n",
    "\n",
    "The epsilon parameter `eps = 1e-8` is \"a very small number to prevent any division by zero in the implementation\" (from [here](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)).\n",
    "\n",
    "You can find the creation of the AdamW optimizer in `run_glue.py` [here](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pablo\\.conda\\envs\\finsent-env\\Lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is our training loop. There's a lot going on, but fundamentally for each pass in our loop we have a trianing phase and a validation phase. \n",
    "\n",
    "> *Thank you to [Stas Bekman](https://ca.linkedin.com/in/stasbekman) for contributing the insights and code for using validation loss to detect over-fitting!*\n",
    "\n",
    "**Training:**\n",
    "- Unpack our data inputs and labels\n",
    "- Load data onto the GPU for acceleration\n",
    "- Clear out the gradients calculated in the previous pass. \n",
    "    - In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out.\n",
    "- Forward pass (feed input data through the network)\n",
    "- Backward pass (backpropagation)\n",
    "- Tell the network to update parameters with optimizer.step()\n",
    "- Track variables for monitoring progress\n",
    "\n",
    "**Evalution:**\n",
    "- Unpack our data inputs and labels\n",
    "- Load data onto the GPU for acceleration\n",
    "- Forward pass (feed input data through the network)\n",
    "- Compute loss on our validation data and track variables for monitoring progress\n",
    "\n",
    "Pytorch hides all of the detailed calculations from us, but we've commented the code to point out which of the above steps are happening on each line. \n",
    "\n",
    "> *PyTorch also has some [beginner tutorials](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py) which you may also find helpful.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # In PyTorch, calling `model` will in turn call the model's `forward` \n",
    "        # function and pass down the arguments. The `forward` function is \n",
    "        # documented here: \n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
    "        # The results are returned in a results object, documented here:\n",
    "        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
    "        # Specifically, we'll get the loss (because we provided labels) and the\n",
    "        # \"logits\"--the model outputs prior to activation.\n",
    "        result = model(b_input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask, \n",
    "                       labels=b_labels,\n",
    "                       return_dict=True)\n",
    "\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            result = model(b_input_ids, \n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=b_input_mask,\n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "\n",
    "        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
    "        # output values prior to applying an activation function like the \n",
    "        # softmax.\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, while the the training loss is going down with each epoch, the validation loss is increasing! This suggests that we are training our model too long, and it's over-fitting on the training data. \n",
    "\n",
    "Validation Loss is a more precise measure than accuracy, because with accuracy we don't care about the exact output value, but just which side of a threshold it falls on. \n",
    "\n",
    "If we are predicting the correct answer, but with less confidence, then validation loss will catch this, while accuracy will not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "output_dir = './model_save/'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Good practice: save your training arguments together with the trained model\n",
    "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 3: Deployment of the sentiment/category tagger on  financial news or social media posts**\n",
    "\n",
    "Let´s now turn our attention to a live deployment of the financial news tagger. Things can get quite complicated, specially if we add streaming data, so it is best to keep the deploymnet lightweight. There are mainly three important pieces. Let´s explore them:\n",
    "\n",
    "\n",
    "- Build a local dashboard/app (e.g. using Streamlit or another web applications framework of your choice). A bit UI to display the sentiment tagger in action and demonstrate the practical application of your model.\n",
    "\n",
    "\n",
    "- Build a financial news/alerts scraper pipeline, filter some entities if you focus your search. In a real world setting,  you’d likely want to build a more robust infrastructure for processing and ingestion of new examples, handling any preprocessing, and outputting predictions. Here are some options where to scrape data (real-time data might be expensive or limited):\n",
    "\n",
    "    - <span style=\"color:blue\">*Social Media Posts*</span>: Pulling historical or live data from tweets or reddit. There are public APIs with extensive documentation for them.\n",
    "    - <span style=\"color:blue\">*OpenBB*</span>: Open research investment platform. It aggregates financial news across the world and has an API to access them.\n",
    "    - <span style=\"color:blue\">*Financial news outlet*</span>: Yahoo Finance\n",
    "    \n",
    "An pipeline example: The basic premise is to read in a stream of tweets, use a lighweight sentiment analysis engine (BERT might not be a good fit here) to assign a bullish/neutral/bearish score to each tweet, and then see how this cumulatively changes over time.\n",
    "    \n",
    "    \n",
    "- Build an inference endpoint for the tagging model. Within your infrastructure, you can deploy and load the resuting model. One way is to build a REST API endpoint, only to be queried locally (in your laptop).\n",
    "\n",
    "\n",
    "\n",
    "**Extra:** You could explore or quantify correlations with the market for a list of selected stock."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
